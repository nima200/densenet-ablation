2018-04-18 18:18:38.302780: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 5205654358157854271
]
Model created
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 28, 28, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 28, 28, 24)   216         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 28, 28, 24)   96          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 28, 28, 24)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 28, 28, 12)   2592        activation_1[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 28, 28, 36)   0           conv2d_1[0][0]                   
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 28, 28, 36)   144         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 28, 28, 36)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 28, 12)   3888        activation_2[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 28, 48)   0           concatenate_1[0][0]              
                                                                 conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 28, 48)   192         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 28, 48)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 28, 12)   5184        activation_3[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 28, 28, 60)   0           concatenate_2[0][0]              
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 28, 60)   240         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 28, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 28, 28, 12)   6480        activation_4[0][0]               
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 28, 72)   0           concatenate_3[0][0]              
                                                                 conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 28, 28, 72)   288         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 28, 28, 72)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 28, 12)   7776        activation_5[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 28, 84)   0           concatenate_4[0][0]              
                                                                 conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 28, 84)   336         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 28, 84)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 12)   9072        activation_6[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 28, 28, 96)   0           concatenate_5[0][0]              
                                                                 conv2d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 96)   384         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 96)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 28, 12)   10368       activation_7[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 28, 28, 108)  0           concatenate_6[0][0]              
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 28, 108)  432         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 28, 108)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 28, 28, 12)   11664       activation_8[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 28, 28, 120)  0           concatenate_7[0][0]              
                                                                 conv2d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 28, 28, 120)  480         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 28, 28, 120)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 28, 28, 12)   12960       activation_9[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 28, 28, 132)  0           concatenate_8[0][0]              
                                                                 conv2d_10[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 28, 28, 132)  528         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 28, 28, 132)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 28, 28, 12)   14256       activation_10[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 28, 28, 144)  0           concatenate_9[0][0]              
                                                                 conv2d_11[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 28, 28, 144)  576         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 28, 28, 144)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 28, 28, 12)   15552       activation_11[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 28, 28, 156)  0           concatenate_10[0][0]             
                                                                 conv2d_12[0][0]                  
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 28, 28, 156)  624         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 28, 28, 156)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 28, 28, 12)   16848       activation_12[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 28, 28, 168)  0           concatenate_11[0][0]             
                                                                 conv2d_13[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 28, 28, 168)  672         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 28, 28, 168)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 28, 28, 168)  28224       activation_13[0][0]              
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 14, 14, 168)  0           conv2d_14[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 14, 14, 168)  672         average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 14, 14, 168)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 14, 14, 12)   18144       activation_14[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 14, 14, 180)  0           average_pooling2d_1[0][0]        
                                                                 conv2d_15[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 14, 14, 180)  720         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 14, 14, 180)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 14, 14, 12)   19440       activation_15[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 14, 14, 192)  0           concatenate_13[0][0]             
                                                                 conv2d_16[0][0]                  
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 14, 14, 192)  768         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 14, 14, 192)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 14, 14, 12)   20736       activation_16[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 14, 14, 204)  0           concatenate_14[0][0]             
                                                                 conv2d_17[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 14, 14, 204)  816         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 14, 14, 204)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 14, 14, 12)   22032       activation_17[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 14, 14, 216)  0           concatenate_15[0][0]             
                                                                 conv2d_18[0][0]                  
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 14, 14, 216)  864         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 14, 14, 216)  0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 14, 14, 12)   23328       activation_18[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 14, 14, 228)  0           concatenate_16[0][0]             
                                                                 conv2d_19[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 14, 14, 228)  912         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 14, 14, 228)  0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 14, 14, 12)   24624       activation_19[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 14, 14, 240)  0           concatenate_17[0][0]             
                                                                 conv2d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 14, 14, 240)  960         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 14, 14, 240)  0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 14, 14, 12)   25920       activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 14, 14, 252)  0           concatenate_18[0][0]             
                                                                 conv2d_21[0][0]                  
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 14, 14, 252)  1008        concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 14, 14, 252)  0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 14, 14, 12)   27216       activation_21[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 14, 14, 264)  0           concatenate_19[0][0]             
                                                                 conv2d_22[0][0]                  
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 14, 14, 264)  1056        concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 14, 14, 264)  0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 14, 14, 12)   28512       activation_22[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 14, 14, 276)  0           concatenate_20[0][0]             
                                                                 conv2d_23[0][0]                  
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 14, 14, 276)  1104        concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 14, 14, 276)  0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 14, 14, 12)   29808       activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 14, 14, 288)  0           concatenate_21[0][0]             
                                                                 conv2d_24[0][0]                  
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 14, 14, 288)  1152        concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 14, 14, 288)  0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 14, 14, 12)   31104       activation_24[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 14, 14, 300)  0           concatenate_22[0][0]             
                                                                 conv2d_25[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 14, 14, 300)  1200        concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 14, 14, 300)  0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 14, 14, 12)   32400       activation_25[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 14, 14, 312)  0           concatenate_23[0][0]             
                                                                 conv2d_26[0][0]                  
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 14, 14, 312)  1248        concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 14, 14, 312)  0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 14, 14, 312)  97344       activation_26[0][0]              
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 7, 7, 312)    0           conv2d_27[0][0]                  
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 7, 7, 312)    1248        average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 7, 7, 312)    0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 7, 7, 12)     33696       activation_27[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 7, 7, 324)    0           average_pooling2d_2[0][0]        
                                                                 conv2d_28[0][0]                  
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 7, 7, 324)    1296        concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 7, 7, 324)    0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 7, 7, 12)     34992       activation_28[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 7, 7, 336)    0           concatenate_25[0][0]             
                                                                 conv2d_29[0][0]                  
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 7, 7, 336)    1344        concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 7, 7, 336)    0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 7, 7, 12)     36288       activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 7, 7, 348)    0           concatenate_26[0][0]             
                                                                 conv2d_30[0][0]                  
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 7, 7, 348)    1392        concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 7, 7, 348)    0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 7, 7, 12)     37584       activation_30[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 7, 7, 360)    0           concatenate_27[0][0]             
                                                                 conv2d_31[0][0]                  
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 7, 7, 360)    1440        concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 7, 7, 360)    0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 7, 7, 12)     38880       activation_31[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 7, 7, 372)    0           concatenate_28[0][0]             
                                                                 conv2d_32[0][0]                  
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 7, 7, 372)    1488        concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 7, 7, 372)    0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 7, 7, 12)     40176       activation_32[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 7, 7, 384)    0           concatenate_29[0][0]             
                                                                 conv2d_33[0][0]                  
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 7, 7, 384)    1536        concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 7, 7, 384)    0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 7, 7, 12)     41472       activation_33[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 7, 7, 396)    0           concatenate_30[0][0]             
                                                                 conv2d_34[0][0]                  
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 7, 7, 396)    1584        concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 7, 7, 396)    0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 7, 7, 12)     42768       activation_34[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 7, 7, 408)    0           concatenate_31[0][0]             
                                                                 conv2d_35[0][0]                  
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 7, 7, 408)    1632        concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 7, 7, 408)    0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 7, 7, 12)     44064       activation_35[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 7, 7, 420)    0           concatenate_32[0][0]             
                                                                 conv2d_36[0][0]                  
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 7, 7, 420)    1680        concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 7, 7, 420)    0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 7, 7, 12)     45360       activation_36[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 7, 7, 432)    0           concatenate_33[0][0]             
                                                                 conv2d_37[0][0]                  
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 7, 7, 432)    1728        concatenate_34[0][0]             
__________________________________________________________________________________________________Traceback (most recent call last):
  File "FashionMNIST/fashion_mnist.py", line 5, in <module>
    import densenet
  File "/home/jamesbrace/ml-ablation/src/FashionMNIST/densenet.py", line 10, in <module>
    from keras.models import Model
ModuleNotFoundError: No module named 'keras.models'
2018-04-18 18:38:02.900772: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-04-18 18:38:02.974971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-04-18 18:38:02.975386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-04-18 18:38:02.975478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2018-04-18 18:38:03.176275: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7

2018-04-18 18:38:03.176648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2018-04-18 18:38:03.216509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 955040488122785197
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 357171200
locality {
  bus_id: 1
}
incarnation: 7322213198532689773
physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7"
]
Model created
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 28, 28, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 28, 28, 24)   216         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 28, 28, 24)   96          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 28, 28, 24)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 28, 28, 12)   2592        activation_1[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 28, 28, 36)   0           conv2d_1[0][0]                   
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 28, 28, 36)   144         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 28, 28, 36)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 28, 12)   3888        activation_2[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 28, 48)   0           concatenate_1[0][0]              
                                                                 conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 28, 48)   192         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 28, 48)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 28, 12)   5184        activation_3[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 28, 28, 60)   0           concatenate_2[0][0]              
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 28, 60)   240         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 28, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 28, 28, 12)   6480        activation_4[0][0]               
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 28, 72)   0           concatenate_3[0][0]              
                                                                 conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 28, 28, 72)   288         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 28, 28, 72)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 28, 12)   7776        activation_5[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 28, 84)   0           concatenate_4[0][0]              
                                                                 conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 28, 84)   336         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 28, 84)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 12)   9072        activation_6[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 28, 28, 96)   0           concatenate_5[0][0]              
                                                                 conv2d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 96)   384         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 96)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 28, 12)   10368       activation_7[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 28, 28, 108)  0           concatenate_6[0][0]              
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 28, 108)  432         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 28, 108)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 28, 28, 12)   11664       activation_8[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 28, 28, 120)  0           concatenate_7[0][0]              
                                                                 conv2d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 28, 28, 120)  480         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 28, 28, 120)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 28, 28, 12)   12960       activation_9[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 28, 28, 132)  0           concatenate_8[0][0]              
                                                                 conv2d_10[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 28, 28, 132)  528         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 28, 28, 132)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 28, 28, 12)   14256       activation_10[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 28, 28, 144)  0           concatenate_9[0][0]              
                                                                 conv2d_11[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 28, 28, 144)  576         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 28, 28, 144)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 28, 28, 12)   15552       activation_11[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 28, 28, 156)  0           concatenate_10[0][0]             
                                                                 conv2d_12[0][0]                  
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 28, 28, 156)  624         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 28, 28, 156)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 28, 28, 12)   16848       activation_12[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 28, 28, 168)  0           concatenate_11[0][0]             
                                                                 conv2d_13[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 28, 28, 168)  672         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 28, 28, 168)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 28, 28, 168)  28224       activation_13[0][0]              
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 14, 14, 168)  0           conv2d_14[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 14, 14, 168)  672         average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 14, 14, 168)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 14, 14, 12)   18144       activation_14[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 14, 14, 180)  0           average_pooling2d_1[0][0]        
                                                                 conv2d_15[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 14, 14, 180)  720         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 14, 14, 180)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 14, 14, 12)   19440       activation_15[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 14, 14, 192)  0           concatenate_13[0][0]             
                                                                 conv2d_16[0][0]                  
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 14, 14, 192)  768         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 14, 14, 192)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 14, 14, 12)   20736       activation_16[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 14, 14, 204)  0           concatenate_14[0][0]             
                                                                 conv2d_17[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 14, 14, 204)  816         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 14, 14, 204)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 14, 14, 12)   22032       activation_17[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 14, 14, 216)  0           concatenate_15[0][0]             
                                                                 conv2d_18[0][0]                  
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 14, 14, 216)  864         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 14, 14, 216)  0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 14, 14, 12)   23328       activation_18[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 14, 14, 228)  0           concatenate_16[0][0]             
                                                                 conv2d_19[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 14, 14, 228)  912         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 14, 14, 228)  0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 14, 14, 12)   24624       activation_19[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 14, 14, 240)  0           concatenate_17[0][0]             
                                                                 conv2d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 14, 14, 240)  960         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 14, 14, 240)  0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 14, 14, 12)   25920       activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 14, 14, 252)  0           concatenate_18[0][0]             
                                                                 conv2d_21[0][0]                  
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 14, 14, 252)  1008        concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 14, 14, 252)  0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 14, 14, 12)   27216       activation_21[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 14, 14, 264)  0           concatenate_19[0][0]             
                                                                 conv2d_22[0][0]                  
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 14, 14, 264)  1056        concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 14, 14, 264)  0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 14, 14, 12)   28512       activation_22[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 14, 14, 276)  0           concatenate_20[0][0]             
                                                                 conv2d_23[0][0]                  
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 14, 14, 276)  1104        concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 14, 14, 276)  0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 14, 14, 12)   29808       activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 14, 14, 288)  0           concatenate_21[0][0]             
                                                                 conv2d_24[0][0]                  
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 14, 14, 288)  1152        concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 14, 14, 288)  0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 14, 14, 12)   31104       activation_24[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 14, 14, 300)  0           concatenate_22[0][0]             
                                                                 conv2d_25[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 14, 14, 300)  1200        concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 14, 14, 300)  0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 14, 14, 12)   32400       activation_25[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 14, 14, 312)  0           concatenate_23[0][0]             
                                                                 conv2d_26[0][0]                  
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 14, 14, 312)  1248        concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 14, 14, 312)  0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 14, 14, 312)  97344       activation_26[0][0]              
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 7, 7, 312)    0           conv2d_27[0][0]                  
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 7, 7, 312)    1248        average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 7, 7, 312)    0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 7, 7, 12)     33696       activation_27[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 7, 7, 324)    0           average_pooling2d_2[0][0]        
                                                                 conv2d_28[0][0]                  
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 7, 7, 324)    1296        concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 7, 7, 324)    0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 7, 7, 12)     34992       activation_28[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 7, 7, 336)    0           concatenate_25[0][0]             
                                                                 conv2d_29[0][0]                  
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 7, 7, 336)    1344        concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 7, 7, 336)    0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 7, 7, 12)     36288       activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 7, 7, 348)    0           concatenate_26[0][0]             
                                                                 conv2d_30[0][0]                  
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 7, 7, 348)    1392        concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 7, 7, 348)    0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 7, 7, 12)     37584       activation_30[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 7, 7, 360)    0           concatenate_27[0][0]             
                                                                 conv2d_31[0][0]                  
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 7, 7, 360)    1440        concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 7, 7, 360)    0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 7, 7, 12)     38880       activation_31[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 7, 7, 372)    0           concatenate_28[0][0]             
                                                                 conv2d_32[0][0]                  
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 7, 7, 372)    1488        concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 7, 7, 372)    0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 7, 7, 12)     40176       activation_32[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 7, 7, 384)    0           concatenate_29[0][0]             
                                                                 conv2d_33[0][0]                  
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 7, 7, 384)    1536        concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 7, 7, 384)    0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 7, 7, 12)     41472       activation_33[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 7, 7, 396)    0           concatenate_30[0][0]             
                                                                 conv2d_34[0][0]                  
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 7, 7, 396)    1584        concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 7, 7, 396)    0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 7, 7, 12)     42768       activation_34[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 7, 7, 408)    0           concatenate_31[0][0]             
                                                                 conv2d_35[0][0]                  
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 7, 7, 408)    1632        concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 7, 7, 408)    0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 7, 7, 12)     44064       activation_35[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 7, 7, 420)    0           concatenate_32[0][0]             
                                                                 conv2d_36[0][0]                  
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 7, 7, 420)    1680        concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 7, 7, 420)    0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 7, 7, 12)     45360       activation_36[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 7, 7, 432)    0           concatenate_33[0][0]             
                                                                 conv2d_37[0][0]                  
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 7, 7, 432)    1728        concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 7, 7, 432)    0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 7, 7, 12)     46656       activation_37[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 7, 7, 444)    0           concatenate_34[0][0]             
                                                                 conv2d_38[0][0]                  
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 7, 7, 444)    1776        concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 7, 7, 444)    0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 7, 7, 12)     47952       activation_38[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 7, 7, 456)    0           concatenate_35[0][0]             
                                                                 conv2d_39[0][0]                  
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 7, 7, 456)    1824        concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 7, 7, 456)    0           batch_normalization_39[0][0]     
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 456)          0           activation_39[0][0]              
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           4570        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 1,077,586
Trainable params: 1,058,866
Non-trainable params: 18,720
__________________________________________________________________________________________________
Finished compiling
Building model...
Training with data augmentation...
Epoch 1/40

  1/600 [..............................] - ETA: 1:03:37 - loss: 2.5375 - acc: 0.0800
  2/600 [..............................] - ETA: 33:54 - loss: 2.3618 - acc: 0.2000  
  3/600 [..............................] - ETA: 23:58 - loss: 2.2483 - acc: 0.2400
  4/600 [..............................] - ETA: 19:01 - loss: 2.1050 - acc: 0.2850
  5/600 [..............................] - ETA: 16:01 - loss: 2.0058 - acc: 0.3060
  6/600 [..............................] - ETA: 14:02 - loss: 1.9337 - acc: 0.3333
  7/600 [..............................] - ETA: 12:36 - loss: 1.8734 - acc: 0.3500
  8/600 [..............................] - ETA: 11:32 - loss: 1.8120 - acc: 0.3800
  9/600 [..............................] - ETA: 10:42 - loss: 1.7741 - acc: 0.3967
 10/600 [..............................] - ETA: 10:02 - loss: 1.7298 - acc: 0.4170
 11/600 [..............................] - ETA: 9:29 - loss: 1.6937 - acc: 0.4318 
 12/600 [..............................] - ETA: 9:01 - loss: 1.6719 - acc: 0.4400
 13/600 [..............................] - ETA: 8:38 - loss: 1.6462 - acc: 0.4469
 14/600 [..............................] - ETA: 8:18 - loss: 1.6199 - acc: 0.4586
 15/600 [..............................] - ETA: 8:01 - loss: 1.5972 - acc: 0.4713
 16/600 [..............................] - ETA: 7:45 - loss: 1.5739 - acc: 0.4831
 17/600 [..............................] - ETA: 7:32 - loss: 1.5561 - acc: 0.4900
 18/600 [..............................] - ETA: 7:20 - loss: 1.5405 - acc: 0.4928
 19/600 [..............................] - ETA: 7:09 - loss: 1.5216 - acc: 0.4995
 20/600 [>.............................] - ETA: 6:59 - loss: 1.5028 - acc: 0.5075
 21/600 [>.............................] - ETA: 6:50 - loss: 1.4862 - acc: 0.5095
 22/600 [>.............................] - ETA: 6:42 - loss: 1.4695 - acc: 0.5159
 23/600 [>.............................] - ETA: 6:35 - loss: 1.4505 - acc: 0.5226
 24/600 [>.............................] - ETA: 6:28 - loss: 1.4374 - acc: 0.5267
 25/600 [>.............................] - ETA: 6:22 - loss: 1.4238 - acc: 0.5296
 26/600 [>.............................] - ETA: 6:16 - loss: 1.4126 - acc: 0.5338
 27/600 [>.............................] - ETA: 6:10 - loss: 1.4058 - acc: 0.5356
 28/600 [>.............................] - ETA: 6:05 - loss: 1.3923 - acc: 0.5429
 29/600 [>.............................] - ETA: 6:00 - loss: 1.3745 - acc: 0.5486
 30/600 [>.............................] - ETA: 5:56 - loss: 1.3635 - acc: 0.5517
 31/600 [>.............................] - ETA: 5:52 - loss: 1.3610 - acc: 0.5532
 32/600 [>.............................] - ETA: 5:48 - loss: 1.3512 - acc: 0.5572
 33/600 [>.............................] - ETA: 5:44 - loss: 1.3422 - acc: 0.5606
 34/600 [>.............................] - ETA: 5:40 - loss: 1.3346 - acc: 0.5635
 35/600 [>.............................] - ETA: 5:37 - loss: 1.3233 - acc: 0.5691
 36/600 [>.............................] - ETA: 5:34 - loss: 1.3143 - acc: 0.5728
 37/600 [>.............................] - ETA: 5:31 - loss: 1.3051 - acc: 0.5754
 38/600 [>.............................] - ETA: 5:28 - loss: 1.2922 - acc: 0.5795
 39/600 [>.............................] - ETA: 5:25 - loss: 1.2817 - acc: 0.5826
 40/600 [=>............................] - ETA: 5:22 - loss: 1.2773 - acc: 0.5845
 41/600 [=>............................] - ETA: 5:19 - loss: 1.2695 - acc: 0.5880
 42/600 [=>............................] - ETA: 5:17 - loss: 1.2595 - acc: 0.5912
 43/600 [=>............................] - ETA: 5:15 - loss: 1.2541 - acc: 0.5935
 44/600 [=>............................] - ETA: 5:12 - loss: 1.2494 - acc: 0.5943
 45/600 [=>............................] - ETA: 5:10 - loss: 1.2401 - acc: 0.5982
 46/600 [=>............................] - ETA: 5:08 - loss: 1.2333 - acc: 0.6002
 47/600 [=>............................] - ETA: 5:06 - loss: 1.2288 - acc: 0.6011
 48/600 [=>............................] - ETA: 5:04 - loss: 1.2206 - acc: 0.6038
 49/600 [=>............................] - ETA: 5:02 - loss: 1.2116 - acc: 0.6067
 50/600 [=>............................] - ETA: 5:00 - loss: 1.2018 - acc: 0.6100
 51/600 [=>............................] - ETA: 4:58 - loss: 1.1971 - acc: 0.6114
 52/600 [=>............................] - ETA: 4:56 - loss: 1.1881 - acc: 0.6146
 53/600 [=>............................] - ETA: 4:55 - loss: 1.1823 - acc: 0.6164
 54/600 [=>............................] - ETA: 4:53 - loss: 1.1753 - acc: 0.6189
 55/600 [=>............................] - ETA: 4:51 - loss: 1.1682 - acc: 0.6204
 56/600 [=>............................] - ETA: 4:50 - loss: 1.1621 - acc: 0.6230
 57/600 [=>............................] - ETA: 4:48 - loss: 1.1564 - acc: 0.6256
 58/600 [=>............................] - ETA: 4:47 - loss: 1.1528 - acc: 0.6271
 59/600 [=>............................] - ETA: 4:45 - loss: 1.1474 - acc: 0.6286
 60/600 [==>...........................] - ETA: 4:44 - loss: 1.1412 - acc: 0.6308
 61/600 [==>...........................] - ETA: 4:42 - loss: 1.1354 - acc: 0.6325
 62/600 [==>...........................] - ETA: 4:41 - loss: 1.1318 - acc: 0.6329
 63/600 [==>...........................] - ETA: 4:40 - loss: 1.1258 - acc: 0.6346
 64/600 [==>...........................] - ETA: 4:38 - loss: 1.1214 - acc: 0.6356
 65/600 [==>...........................] - ETA: 4:37 - loss: 1.1180 - acc: 0.6372
 66/600 [==>...........................] - ETA: 4:36 - loss: 1.1107 - acc: 0.6400
 67/600 [==>...........................] - ETA: 4:34 - loss: 1.1055 - acc: 0.6421
 68/600 [==>...........................] - ETA: 4:33 - loss: 1.0995 - acc: 0.6438
 69/600 [==>...........................] - ETA: 4:32 - loss: 1.0946 - acc: 0.6461
 70/600 [==>...........................] - ETA: 4:31 - loss: 1.0904 - acc: 0.6473
 71/600 [==>...........................] - ETA: 4:30 - loss: 1.0846 - acc: 0.6485
 72/600 [==>...........................] - ETA: 4:29 - loss: 1.0811 - acc: 0.6493
 73/600 [==>...........................] - ETA: 4:27 - loss: 1.0775 - acc: 0.6504
 74/600 [==>...........................] - ETA: 4:26 - loss: 1.0742 - acc: 0.6509
 75/600 [==>...........................] - ETA: 4:25 - loss: 1.0695 - acc: 0.6528
 76/600 [==>...........................] - ETA: 4:24 - loss: 1.0660 - acc: 0.6538
 77/600 [==>...........................] - ETA: 4:23 - loss: 1.0609 - acc: 0.6555
 78/600 [==>...........................] - ETA: 4:22 - loss: 1.0597 - acc: 0.6559
 79/600 [==>...........................] - ETA: 4:21 - loss: 1.0556 - acc: 0.6573
 80/600 [===>..........................] - ETA: 4:20 - loss: 1.0523 - acc: 0.6579
 81/600 [===>..........................] - ETA: 4:19 - loss: 1.0510 - acc: 0.6580
 82/600 [===>..........................] - ETA: 4:18 - loss: 1.0477 - acc: 0.6594
 83/600 [===>..........................] - ETA: 4:17 - loss: 1.0424 - acc: 0.6616
 84/600 [===>..........................] - ETA: 4:16 - loss: 1.0381 - acc: 0.6630
 85/600 [===>..........................] - ETA: 4:15 - loss: 1.0335 - acc: 0.6646
 86/600 [===>..........................] - ETA: 4:14 - loss: 1.0319 - acc: 0.6655
 87/600 [===>..........................] - ETA: 4:14 - loss: 1.0284 - acc: 0.6662
 88/600 [===>..........................] - ETA: 4:13 - loss: 1.0282 - acc: 0.6656
 89/600 [===>..........................] - ETA: 4:12 - loss: 1.0247 - acc: 0.6664
 90/600 [===>..........................] - ETA: 4:11 - loss: 1.0206 - acc: 0.6678
 91/600 [===>..........................] - ETA: 4:10 - loss: 1.0190 - acc: 0.6682
 92/600 [===>..........................] - ETA: 4:09 - loss: 1.0184 - acc: 0.6685
 93/600 [===>..........................] - ETA: 4:08 - loss: 1.0157 - acc: 0.6690
 94/600 [===>..........................] - ETA: 4:08 - loss: 1.0126 - acc: 0.6702
 95/600 [===>..........................] - ETA: 4:07 - loss: 1.0099 - acc: 0.6713
 96/600 [===>..........................] - ETA: 4:06 - loss: 1.0065 - acc: 0.6725
 97/600 [===>..........................] - ETA: 4:05 - loss: 1.0037 - acc: 0.6733
 98/600 [===>..........................] - ETA: 4:04 - loss: 1.0024 - acc: 0.6738
 99/600 [===>..........................] - ETA: 4:03 - loss: 1.0002 - acc: 0.6743
100/600 [====>.........................] - ETA: 4:03 - loss: 0.9977 - acc: 0.6752
101/600 [====>.........................] - ETA: 4:02 - loss: 0.9948 - acc: 0.6760
102/600 [====>.........................] - ETA: 4:01 - loss: 0.9924 - acc: 0.6771
103/600 [====>.........................] - ETA: 4:00 - loss: 0.9907 - acc: 0.6778
104/600 [====>.........................] - ETA: 4:00 - loss: 0.9887 - acc: 0.6785
105/600 [====>.........................] - ETA: 3:59 - loss: 0.9883 - acc: 0.6782
106/600 [====>.........................] - ETA: 3:58 - loss: 0.9872 - acc: 0.6786
107/600 [====>.........................] - ETA: 3:57 - loss: 0.9835 - acc: 0.6802
108/600 [====>.........................] - ETA: 3:57 - loss: 0.9815 - acc: 0.6809
109/600 [====>.........................] - ETA: 3:56 - loss: 0.9806 - acc: 0.6810
110/600 [====>.........................] - ETA: 3:55 - loss: 0.9785 - acc: 0.6817
111/600 [====>.........................] - ETA: 3:54 - loss: 0.9788 - acc: 0.6819
112/600 [====>.........................] - ETA: 3:54 - loss: 0.9777 - acc: 0.6823
113/600 [====>.........................] - ETA: 3:53 - loss: 0.9739 - acc: 0.6838
114/600 [====>.........................] - ETA: 3:52 - loss: 0.9725 - acc: 0.6843
115/600 [====>.........................] - ETA: 3:52 - loss: 0.9712 - acc: 0.6846
116/600 [====>.........................] - ETA: 3:51 - loss: 0.9679 - acc: 0.6858
117/600 [====>.........................] - ETA: 3:50 - loss: 0.9651 - acc: 0.6869
118/600 [====>.........................] - ETA: 3:50 - loss: 0.9642 - acc: 0.6876
119/600 [====>.........................] - ETA: 3:49 - loss: 0.9622 - acc: 0.6882
120/600 [=====>........................] - ETA: 3:48 - loss: 0.9597 - acc: 0.6889
121/600 [=====>........................] - ETA: 3:48 - loss: 0.9577 - acc: 0.6896
122/600 [=====>........................] - ETA: 3:47 - loss: 0.9555 - acc: 0.6902
123/600 [=====>........................] - ETA: 3:46 - loss: 0.9532 - acc: 0.6907
124/600 [=====>........................] - ETA: 3:46 - loss: 0.9519 - acc: 0.6912
125/600 [=====>........................] - ETA: 3:45 - loss: 0.9500 - acc: 0.6914
126/600 [=====>........................] - ETA: 3:44 - loss: 0.9473 - acc: 0.6924
127/600 [=====>........................] - ETA: 3:44 - loss: 0.9463 - acc: 0.6928
128/600 [=====>........................] - ETA: 3:43 - loss: 0.9444 - acc: 0.6935
129/600 [=====>........................] - ETA: 3:42 - loss: 0.9423 - acc: 0.6943
130/600 [=====>........................] - ETA: 3:42 - loss: 0.9404 - acc: 0.6951
131/600 [=====>........................] - ETA: 3:41 - loss: 0.9383 - acc: 0.6954
132/600 [=====>........................] - ETA: 3:40 - loss: 0.9369 - acc: 0.6956
133/600 [=====>........................] - ETA: 3:40 - loss: 0.9340 - acc: 0.6964
134/600 [=====>........................] - ETA: 3:39 - loss: 0.9328 - acc: 0.6966
135/600 [=====>........................] - ETA: 3:38 - loss: 0.9317 - acc: 0.6965
136/600 [=====>........................] - ETA: 3:38 - loss: 0.9297 - acc: 0.6970
137/600 [=====>........................] - ETA: 3:37 - loss: 0.9284 - acc: 0.6970
138/600 [=====>........................] - ETA: 3:37 - loss: 0.9258 - acc: 0.6980
139/600 [=====>........................] - ETA: 3:36 - loss: 0.9237 - acc: 0.6988
140/600 [======>.......................] - ETA: 3:35 - loss: 0.9221 - acc: 0.6992
141/600 [======>.......................] - ETA: 3:35 - loss: 0.9205 - acc: 0.6999
142/600 [======>.......................] - ETA: 3:34 - loss: 0.9198 - acc: 0.7002
143/600 [======>.......................] - ETA: 3:34 - loss: 0.9194 - acc: 0.7006
144/600 [======>.......................] - ETA: 3:33 - loss: 0.9179 - acc: 0.7013
145/600 [======>.......................] - ETA: 3:32 - loss: 0.9161 - acc: 0.7018
146/600 [======>.......................] - ETA: 3:32 - loss: 0.9144 - acc: 0.7025
147/600 [======>.......................] - ETA: 3:31 - loss: 0.9129 - acc: 0.7030
148/600 [======>.......................] - ETA: 3:31 - loss: 0.9113 - acc: 0.7036
149/600 [======>.......................] - ETA: 3:30 - loss: 0.9105 - acc: 0.7040
150/600 [======>.......................] - ETA: 3:29 - loss: 0.9092 - acc: 0.7043
151/600 [======>.......................] - ETA: 3:29 - loss: 0.9084 - acc: 0.7044
152/600 [======>.......................] - ETA: 3:28 - loss: 0.9071 - acc: 0.7046
153/600 [======>.......................] - ETA: 3:28 - loss: 0.9055 - acc: 0.7046
154/600 [======>.......................] - ETA: 3:27 - loss: 0.9033 - acc: 0.7054
155/600 [======>.......................] - ETA: 3:27 - loss: 0.9018 - acc: 0.7059
156/600 [======>.......................] - ETA: 3:26 - loss: 0.9004 - acc: 0.7064
157/600 [======>.......................] - ETA: 3:25 - loss: 0.8990 - acc: 0.7069
158/600 [======>.......................] - ETA: 3:25 - loss: 0.8970 - acc: 0.7073
159/600 [======>.......................] - ETA: 3:24 - loss: 0.8960 - acc: 0.7075
160/600 [=======>......................] - ETA: 3:24 - loss: 0.8944 - acc: 0.7081
161/600 [=======>......................] - ETA: 3:23 - loss: 0.8930 - acc: 0.7084
162/600 [=======>......................] - ETA: 3:23 - loss: 0.8915 - acc: 0.7092
163/600 [=======>......................] - ETA: 3:22 - loss: 0.8897 - acc: 0.7099
164/600 [=======>......................] - ETA: 3:21 - loss: 0.8884 - acc: 0.7105
165/600 [=======>......................] - ETA: 3:21 - loss: 0.8862 - acc: 0.7113
166/600 [=======>......................] - ETA: 3:20 - loss: 0.8847 - acc: 0.7119
167/600 [=======>......................] - ETA: 3:20 - loss: 0.8841 - acc: 0.7121
168/600 [=======>......................] - ETA: 3:19 - loss: 0.8826 - acc: 0.7128
169/600 [=======>......................] - ETA: 3:19 - loss: 0.8818 - acc: 0.7128
170/600 [=======>......................] - ETA: 3:18 - loss: 0.8807 - acc: 0.7131
171/600 [=======>......................] - ETA: 3:18 - loss: 0.8790 - acc: 0.7136
172/600 [=======>......................] - ETA: 3:17 - loss: 0.8780 - acc: 0.7138
173/600 [=======>......................] - ETA: 3:16 - loss: 0.8769 - acc: 0.7143
174/600 [=======>......................] - ETA: 3:16 - loss: 0.8750 - acc: 0.7148
175/600 [=======>......................] - ETA: 3:15 - loss: 0.8736 - acc: 0.7151
176/600 [=======>......................] - ETA: 3:15 - loss: 0.8725 - acc: 0.7155
177/600 [=======>......................] - ETA: 3:14 - loss: 0.8717 - acc: 0.7156
178/600 [=======>......................] - ETA: 3:14 - loss: 0.8707 - acc: 0.7161
179/600 [=======>......................] - ETA: 3:13 - loss: 0.8697 - acc: 0.7165
180/600 [========>.....................] - ETA: 3:13 - loss: 0.8683 - acc: 0.7170
181/600 [========>.....................] - ETA: 3:12 - loss: 0.8674 - acc: 0.7173
182/600 [========>.....................] - ETA: 3:12 - loss: 0.8652 - acc: 0.7182
183/600 [========>.....................] - ETA: 3:11 - loss: 0.8648 - acc: 0.7184
184/600 [========>.....................] - ETA: 3:11 - loss: 0.8636 - acc: 0.7189
185/600 [========>.....................] - ETA: 3:10 - loss: 0.8627 - acc: 0.7191
186/600 [========>.....................] - ETA: 3:09 - loss: 0.8615 - acc: 0.7196
187/600 [========>.....................] - ETA: 3:09 - loss: 0.8605 - acc: 0.7200
188/600 [========>.....................] - ETA: 3:08 - loss: 0.8599 - acc: 0.7202
189/600 [========>.....................] - ETA: 3:08 - loss: 0.8595 - acc: 0.7205
190/600 [========>.....................] - ETA: 3:07 - loss: 0.8581 - acc: 0.7211
191/600 [========>.....................] - ETA: 3:07 - loss: 0.8578 - acc: 0.7210
192/600 [========>.....................] - ETA: 3:06 - loss: 0.8568 - acc: 0.7215
193/600 [========>.....................] - ETA: 3:06 - loss: 0.8559 - acc: 0.7217
194/600 [========>.....................] - ETA: 3:05 - loss: 0.8550 - acc: 0.7221
195/600 [========>.....................] - ETA: 3:05 - loss: 0.8541 - acc: 0.7227
196/600 [========>.....................] - ETA: 3:04 - loss: 0.8530 - acc: 0.7231
197/600 [========>.....................] - ETA: 3:04 - loss: 0.8525 - acc: 0.7235
198/600 [========>.....................] - ETA: 3:03 - loss: 0.8510 - acc: 0.7240
199/600 [========>.....................] - ETA: 3:03 - loss: 0.8501 - acc: 0.7241
200/600 [=========>....................] - ETA: 3:02 - loss: 0.8491 - acc: 0.7245
201/600 [=========>....................] - ETA: 3:02 - loss: 0.8481 - acc: 0.7246
202/600 [=========>....................] - ETA: 3:01 - loss: 0.8472 - acc: 0.7248
203/600 [=========>....................] - ETA: 3:01 - loss: 0.8468 - acc: 0.7247
204/600 [=========>....................] - ETA: 3:00 - loss: 0.8455 - acc: 0.7251
205/600 [=========>....................] - ETA: 3:00 - loss: 0.8445 - acc: 0.7254
206/600 [=========>....................] - ETA: 2:59 - loss: 0.8435 - acc: 0.7256
207/600 [=========>....................] - ETA: 2:59 - loss: 0.8430 - acc: 0.7256
208/600 [=========>....................] - ETA: 2:58 - loss: 0.8421 - acc: 0.7259
209/600 [=========>....................] - ETA: 2:58 - loss: 0.8409 - acc: 0.7263
210/600 [=========>....................] - ETA: 2:57 - loss: 0.8392 - acc: 0.7270
211/600 [=========>....................] - ETA: 2:57 - loss: 0.8381 - acc: 0.7272
212/600 [=========>....................] - ETA: 2:56 - loss: 0.8366 - acc: 0.7277
213/600 [=========>....................] - ETA: 2:55 - loss: 0.8355 - acc: 0.7280
214/600 [=========>....................] - ETA: 2:55 - loss: 0.8348 - acc: 0.7282
215/600 [=========>....................] - ETA: 2:54 - loss: 0.8335 - acc: 0.7287
216/600 [=========>....................] - ETA: 2:54 - loss: 0.8325 - acc: 0.7290
217/600 [=========>....................] - ETA: 2:53 - loss: 0.8317 - acc: 0.7294
218/600 [=========>....................] - ETA: 2:53 - loss: 0.8305 - acc: 0.7299
219/600 [=========>....................] - ETA: 2:52 - loss: 0.8302 - acc: 0.7300
220/600 [==========>...................] - ETA: 2:52 - loss: 0.8286 - acc: 0.7305
221/600 [==========>...................] - ETA: 2:51 - loss: 0.8278 - acc: 0.7308
222/600 [==========>...................] - ETA: 2:51 - loss: 0.8276 - acc: 0.7310
223/600 [==========>...................] - ETA: 2:50 - loss: 0.8269 - acc: 0.7311
224/600 [==========>...................] - ETA: 2:50 - loss: 0.8258 - acc: 0.7315
225/600 [==========>...................] - ETA: 2:49 - loss: 0.8249 - acc: 0.7316
226/600 [==========>...................] - ETA: 2:49 - loss: 0.8234 - acc: 0.7319
227/600 [==========>...................] - ETA: 2:48 - loss: 0.8228 - acc: 0.7321
228/600 [==========>...................] - ETA: 2:48 - loss: 0.8221 - acc: 0.7323
229/600 [==========>...................] - ETA: 2:47 - loss: 0.8211 - acc: 0.7329
230/600 [==========>...................] - ETA: 2:47 - loss: 0.8198 - acc: 0.7333
231/600 [==========>...................] - ETA: 2:46 - loss: 0.8189 - acc: 0.7336
232/600 [==========>...................] - ETA: 2:46 - loss: 0.8185 - acc: 0.7337
233/600 [==========>...................] - ETA: 2:46 - loss: 0.8178 - acc: 0.7340
234/600 [==========>...................] - ETA: 2:45 - loss: 0.8169 - acc: 0.7342
235/600 [==========>...................] - ETA: 2:45 - loss: 0.8159 - acc: 0.7346
236/600 [==========>...................] - ETA: 2:44 - loss: 0.8157 - acc: 0.7344
237/600 [==========>...................] - ETA: 2:44 - loss: 0.8151 - acc: 0.7348
238/600 [==========>...................] - ETA: 2:43 - loss: 0.8143 - acc: 0.7350
239/600 [==========>...................] - ETA: 2:43 - loss: 0.8136 - acc: 0.7351
240/600 [===========>..................] - ETA: 2:42 - loss: 0.8129 - acc: 0.7354
241/600 [===========>..................] - ETA: 2:42 - loss: 0.8119 - acc: 0.7358
242/600 [===========>..................] - ETA: 2:41 - loss: 0.8110 - acc: 0.7360
243/600 [===========>..................] - ETA: 2:41 - loss: 0.8099 - acc: 0.7366
244/600 [===========>..................] - ETA: 2:40 - loss: 0.8088 - acc: 0.7370
245/600 [===========>..................] - ETA: 2:40 - loss: 0.8081 - acc: 0.7372
246/600 [===========>..................] - ETA: 2:39 - loss: 0.8080 - acc: 0.7373
247/600 [===========>..................] - ETA: 2:39 - loss: 0.8073 - acc: 0.7374
248/600 [===========>..................] - ETA: 2:38 - loss: 0.8061 - acc: 0.7380
249/600 [===========>..................] - ETA: 2:38 - loss: 0.8053 - acc: 0.7384
250/600 [===========>..................] - ETA: 2:37 - loss: 0.8043 - acc: 0.7387
251/600 [===========>..................] - ETA: 2:37 - loss: 0.8044 - acc: 0.7386
252/600 [===========>..................] - ETA: 2:36 - loss: 0.8037 - acc: 0.7388
253/600 [===========>..................] - ETA: 2:36 - loss: 0.8030 - acc: 0.7390
254/600 [===========>..................] - ETA: 2:35 - loss: 0.8020 - acc: 0.7394
255/600 [===========>..................] - ETA: 2:35 - loss: 0.8015 - acc: 0.7395
256/600 [===========>..................] - ETA: 2:34 - loss: 0.8007 - acc: 0.7397
257/600 [===========>..................] - ETA: 2:34 - loss: 0.8008 - acc: 0.7398
258/600 [===========>..................] - ETA: 2:33 - loss: 0.7998 - acc: 0.7402
259/600 [===========>..................] - ETA: 2:33 - loss: 0.7989 - acc: 0.7404
260/600 [============>.................] - ETA: 2:32 - loss: 0.7983 - acc: 0.7407
261/600 [============>.................] - ETA: 2:32 - loss: 0.7981 - acc: 0.7408
262/600 [============>.................] - ETA: 2:31 - loss: 0.7971 - acc: 0.7411
263/600 [============>.................] - ETA: 2:31 - loss: 0.7966 - acc: 0.7413
264/600 [============>.................] - ETA: 2:30 - loss: 0.7963 - acc: 0.7414
265/600 [============>.................] - ETA: 2:30 - loss: 0.7957 - acc: 0.7415
266/600 [============>.................] - ETA: 2:29 - loss: 0.7959 - acc: 0.7416
267/600 [============>.................] - ETA: 2:29 - loss: 0.7950 - acc: 0.7419
268/600 [============>.................] - ETA: 2:29 - loss: 0.7946 - acc: 0.7420
269/600 [============>.................] - ETA: 2:28 - loss: 0.7935 - acc: 0.7424
270/600 [============>.................] - ETA: 2:28 - loss: 0.7930 - acc: 0.7426
271/600 [============>.................] - ETA: 2:27 - loss: 0.7919 - acc: 0.7429
272/600 [============>.................] - ETA: 2:27 - loss: 0.7909 - acc: 0.7432
273/600 [============>.................] - ETA: 2:26 - loss: 0.7901 - acc: 0.7435
274/600 [============>.................] - ETA: 2:26 - loss: 0.7898 - acc: 0.7435
275/600 [============>.................] - ETA: 2:25 - loss: 0.7887 - acc: 0.7439
276/600 [============>.................] - ETA: 2:25 - loss: 0.7878 - acc: 0.7441
277/600 [============>.................] - ETA: 2:24 - loss: 0.7870 - acc: 0.7444
278/600 [============>.................] - ETA: 2:24 - loss: 0.7866 - acc: 0.7445
279/600 [============>.................] - ETA: 2:23 - loss: 0.7865 - acc: 0.7446
280/600 [=============>................] - ETA: 2:23 - loss: 0.7855 - acc: 0.7450
281/600 [=============>................] - ETA: 2:22 - loss: 0.7844 - acc: 0.7455
282/600 [=============>................] - ETA: 2:22 - loss: 0.7835 - acc: 0.7459
283/600 [=============>................] - ETA: 2:21 - loss: 0.7830 - acc: 0.7460
284/600 [=============>................] - ETA: 2:21 - loss: 0.7825 - acc: 0.7461
285/600 [=============>................] - ETA: 2:20 - loss: 0.7818 - acc: 0.7464
286/600 [=============>................] - ETA: 2:20 - loss: 0.7813 - acc: 0.7466
287/600 [=============>................] - ETA: 2:20 - loss: 0.7810 - acc: 0.7467
288/600 [=============>................] - ETA: 2:19 - loss: 0.7801 - acc: 0.7470
289/600 [=============>................] - ETA: 2:19 - loss: 0.7791 - acc: 0.7473
290/600 [=============>................] - ETA: 2:18 - loss: 0.7781 - acc: 0.7477
291/600 [=============>................] - ETA: 2:18 - loss: 0.7772 - acc: 0.7480
292/600 [=============>................] - ETA: 2:17 - loss: 0.7764 - acc: 0.7482
293/600 [=============>................] - ETA: 2:17 - loss: 0.7760 - acc: 0.7483
294/600 [=============>................] - ETA: 2:16 - loss: 0.7750 - acc: 0.7486
295/600 [=============>................] - ETA: 2:16 - loss: 0.7742 - acc: 0.7489
296/600 [=============>................] - ETA: 2:15 - loss: 0.7735 - acc: 0.7492
297/600 [=============>................] - ETA: 2:15 - loss: 0.7731 - acc: 0.7493
298/600 [=============>................] - ETA: 2:14 - loss: 0.7728 - acc: 0.7492
299/600 [=============>................] - ETA: 2:14 - loss: 0.7723 - acc: 0.7492
300/600 [==============>...............] - ETA: 2:13 - loss: 0.7724 - acc: 0.7492
301/600 [==============>...............] - ETA: 2:13 - loss: 0.7714 - acc: 0.7496
302/600 [==============>...............] - ETA: 2:13 - loss: 0.7706 - acc: 0.7499
303/600 [==============>...............] - ETA: 2:12 - loss: 0.7700 - acc: 0.7502
304/600 [==============>...............] - ETA: 2:12 - loss: 0.7698 - acc: 0.7502
305/600 [==============>...............] - ETA: 2:11 - loss: 0.7690 - acc: 0.7505
306/600 [==============>...............] - ETA: 2:11 - loss: 0.7682 - acc: 0.7507
307/600 [==============>...............] - ETA: 2:10 - loss: 0.7682 - acc: 0.7507
308/600 [==============>...............] - ETA: 2:10 - loss: 0.7682 - acc: 0.7506
309/600 [==============>...............] - ETA: 2:09 - loss: 0.7677 - acc: 0.7508
310/600 [==============>...............] - ETA: 2:09 - loss: 0.7668 - acc: 0.7510
311/600 [==============>...............] - ETA: 2:08 - loss: 0.7665 - acc: 0.7510
312/600 [==============>...............] - ETA: 2:08 - loss: 0.7662 - acc: 0.7511
313/600 [==============>...............] - ETA: 2:07 - loss: 0.7660 - acc: 0.7511
314/600 [==============>...............] - ETA: 2:07 - loss: 0.7652 - acc: 0.7514
315/600 [==============>...............] - ETA: 2:06 - loss: 0.7645 - acc: 0.7517
316/600 [==============>...............] - ETA: 2:06 - loss: 0.7642 - acc: 0.7517
317/600 [==============>...............] - ETA: 2:06 - loss: 0.7639 - acc: 0.7517
318/600 [==============>...............] - ETA: 2:05 - loss: 0.7635 - acc: 0.7519
319/600 [==============>...............] - ETA: 2:05 - loss: 0.7626 - acc: 0.7523
320/600 [===============>..............] - ETA: 2:04 - loss: 0.7619 - acc: 0.7526
321/600 [===============>..............] - ETA: 2:04 - loss: 0.7616 - acc: 0.7526
322/600 [===============>..............] - ETA: 2:03 - loss: 0.7608 - acc: 0.7529
323/600 [===============>..............] - ETA: 2:03 - loss: 0.7598 - acc: 0.7533
324/600 [===============>..............] - ETA: 2:02 - loss: 0.7592 - acc: 0.7535
325/600 [===============>..............] - ETA: 2:02 - loss: 0.7591 - acc: 0.7537
326/600 [===============>..............] - ETA: 2:01 - loss: 0.7589 - acc: 0.7537
327/600 [===============>..............] - ETA: 2:01 - loss: 0.7582 - acc: 0.7540
328/600 [===============>..............] - ETA: 2:00 - loss: 0.7576 - acc: 0.7543
329/600 [===============>..............] - ETA: 2:00 - loss: 0.7571 - acc: 0.7544
330/600 [===============>..............] - ETA: 2:00 - loss: 0.7568 - acc: 0.7546
331/600 [===============>..............] - ETA: 1:59 - loss: 0.7562 - acc: 0.7549
332/600 [===============>..............] - ETA: 1:59 - loss: 0.7558 - acc: 0.7549
333/600 [===============>..............] - ETA: 1:58 - loss: 0.7553 - acc: 0.7552
334/600 [===============>..............] - ETA: 1:58 - loss: 0.7544 - acc: 0.7554
335/600 [===============>..............] - ETA: 1:57 - loss: 0.7538 - acc: 0.7556
336/600 [===============>..............] - ETA: 1:57 - loss: 0.7538 - acc: 0.7556
337/600 [===============>..............] - ETA: 1:56 - loss: 0.7531 - acc: 0.7558
338/600 [===============>..............] - ETA: 1:56 - loss: 0.7527 - acc: 0.7559
339/600 [===============>..............] - ETA: 1:55 - loss: 0.7519 - acc: 0.7562
340/600 [================>.............] - ETA: 1:55 - loss: 0.7515 - acc: 0.7563
341/600 [================>.............] - ETA: 1:55 - loss: 0.7511 - acc: 0.7565
342/600 [================>.............] - ETA: 1:54 - loss: 0.7505 - acc: 0.7567
343/600 [================>.............] - ETA: 1:54 - loss: 0.7505 - acc: 0.7567
344/600 [================>.............] - ETA: 1:53 - loss: 0.7498 - acc: 0.7570
345/600 [================>.............] - ETA: 1:53 - loss: 0.7491 - acc: 0.7572
346/600 [================>.............] - ETA: 1:52 - loss: 0.7485 - acc: 0.7574
347/600 [================>.............] - ETA: 1:52 - loss: 0.7485 - acc: 0.7574
348/600 [================>.............] - ETA: 1:51 - loss: 0.7484 - acc: 0.7574
349/600 [================>.............] - ETA: 1:51 - loss: 0.7481 - acc: 0.7575
350/600 [================>.............] - ETA: 1:50 - loss: 0.7478 - acc: 0.7575
351/600 [================>.............] - ETA: 1:50 - loss: 0.7475 - acc: 0.7577
352/600 [================>.............] - ETA: 1:49 - loss: 0.7470 - acc: 0.7579
353/600 [================>.............] - ETA: 1:49 - loss: 0.7470 - acc: 0.7578
354/600 [================>.............] - ETA: 1:49 - loss: 0.7467 - acc: 0.7579
355/600 [================>.............] - ETA: 1:48 - loss: 0.7465 - acc: 0.7580
356/600 [================>.............] - ETA: 1:48 - loss: 0.7461 - acc: 0.7581
357/600 [================>.............] - ETA: 1:47 - loss: 0.7457 - acc: 0.7582
358/600 [================>.............] - ETA: 1:47 - loss: 0.7449 - acc: 0.7585
359/600 [================>.............] - ETA: 1:46 - loss: 0.7443 - acc: 0.7587
360/600 [=================>............] - ETA: 1:46 - loss: 0.7438 - acc: 0.7589
361/600 [=================>............] - ETA: 1:45 - loss: 0.7434 - acc: 0.7589
362/600 [=================>............] - ETA: 1:45 - loss: 0.7430 - acc: 0.7590
363/600 [=================>............] - ETA: 1:44 - loss: 0.7422 - acc: 0.7593
364/600 [=================>............] - ETA: 1:44 - loss: 0.7417 - acc: 0.7595
365/600 [=================>............] - ETA: 1:44 - loss: 0.7410 - acc: 0.7598
366/600 [=================>............] - ETA: 1:43 - loss: 0.7401 - acc: 0.7601
367/600 [=================>............] - ETA: 1:43 - loss: 0.7396 - acc: 0.7602
368/600 [=================>............] - ETA: 1:42 - loss: 0.7390 - acc: 0.7604
369/600 [=================>............] - ETA: 1:42 - loss: 0.7389 - acc: 0.7604
370/600 [=================>............] - ETA: 1:41 - loss: 0.7384 - acc: 0.7604
371/600 [=================>............] - ETA: 1:41 - loss: 0.7382 - acc: 0.7605
372/600 [=================>............] - ETA: 1:40 - loss: 0.7373 - acc: 0.7609
373/600 [=================>............] - ETA: 1:40 - loss: 0.7367 - acc: 0.7609
374/600 [=================>............] - ETA: 1:39 - loss: 0.7360 - acc: 0.7611
375/600 [=================>............] - ETA: 1:39 - loss: 0.7354 - acc: 0.7614
376/600 [=================>............] - ETA: 1:39 - loss: 0.7349 - acc: 0.7616
377/600 [=================>............] - ETA: 1:38 - loss: 0.7343 - acc: 0.7619
378/600 [=================>............] - ETA: 1:38 - loss: 0.7340 - acc: 0.7620
379/600 [=================>............] - ETA: 1:37 - loss: 0.7335 - acc: 0.7621
380/600 [==================>...........] - ETA: 1:37 - loss: 0.7332 - acc: 0.7622
381/600 [==================>...........] - ETA: 1:36 - loss: 0.7326 - acc: 0.7624
382/600 [==================>...........] - ETA: 1:36 - loss: 0.7321 - acc: 0.7625
383/600 [==================>...........] - ETA: 1:35 - loss: 0.7314 - acc: 0.7628
384/600 [==================>...........] - ETA: 1:35 - loss: 0.7312 - acc: 0.7629
385/600 [==================>...........] - ETA: 1:35 - loss: 0.7309 - acc: 0.7630
386/600 [==================>...........] - ETA: 1:34 - loss: 0.7305 - acc: 0.7632
387/600 [==================>...........] - ETA: 1:34 - loss: 0.7304 - acc: 0.7632
388/600 [==================>...........] - ETA: 1:33 - loss: 0.7297 - acc: 0.7635
389/600 [==================>...........] - ETA: 1:33 - loss: 0.7297 - acc: 0.7633
390/600 [==================>...........] - ETA: 1:32 - loss: 0.7294 - acc: 0.7634
391/600 [==================>...........] - ETA: 1:32 - loss: 0.7291 - acc: 0.7636
392/600 [==================>...........] - ETA: 1:31 - loss: 0.7285 - acc: 0.7638
393/600 [==================>...........] - ETA: 1:31 - loss: 0.7284 - acc: 0.7637
394/600 [==================>...........] - ETA: 1:30 - loss: 0.7280 - acc: 0.7639
395/600 [==================>...........] - ETA: 1:30 - loss: 0.7279 - acc: 0.7639
396/600 [==================>...........] - ETA: 1:30 - loss: 0.7274 - acc: 0.7640
397/600 [==================>...........] - ETA: 1:29 - loss: 0.7270 - acc: 0.7641
398/600 [==================>...........] - ETA: 1:29 - loss: 0.7263 - acc: 0.7643
399/600 [==================>...........] - ETA: 1:28 - loss: 0.7259 - acc: 0.7645
400/600 [===================>..........] - ETA: 1:28 - loss: 0.7255 - acc: 0.7646
401/600 [===================>..........] - ETA: 1:27 - loss: 0.7251 - acc: 0.7646
402/600 [===================>..........] - ETA: 1:27 - loss: 0.7249 - acc: 0.7646
403/600 [===================>..........] - ETA: 1:26 - loss: 0.7243 - acc: 0.7648
404/600 [===================>..........] - ETA: 1:26 - loss: 0.7237 - acc: 0.7650
405/600 [===================>..........] - ETA: 1:26 - loss: 0.7232 - acc: 0.7651
406/600 [===================>..........] - ETA: 1:25 - loss: 0.7229 - acc: 0.7653
407/600 [===================>..........] - ETA: 1:25 - loss: 0.7225 - acc: 0.7654
408/600 [===================>..........] - ETA: 1:24 - loss: 0.7221 - acc: 0.7655
409/600 [===================>..........] - ETA: 1:24 - loss: 0.7215 - acc: 0.7656
410/600 [===================>..........] - ETA: 1:23 - loss: 0.7213 - acc: 0.7657
411/600 [===================>..........] - ETA: 1:23 - loss: 0.7209 - acc: 0.7659
412/600 [===================>..........] - ETA: 1:22 - loss: 0.7203 - acc: 0.7661
413/600 [===================>..........] - ETA: 1:22 - loss: 0.7199 - acc: 0.7662
414/600 [===================>..........] - ETA: 1:22 - loss: 0.7194 - acc: 0.7664
415/600 [===================>..........] - ETA: 1:21 - loss: 0.7189 - acc: 0.7666
416/600 [===================>..........] - ETA: 1:21 - loss: 0.7191 - acc: 0.7665
417/600 [===================>..........] - ETA: 1:20 - loss: 0.7187 - acc: 0.7667
418/600 [===================>..........] - ETA: 1:20 - loss: 0.7181 - acc: 0.7669
419/600 [===================>..........] - ETA: 1:19 - loss: 0.7177 - acc: 0.7671
420/600 [====================>.........] - ETA: 1:19 - loss: 0.7176 - acc: 0.7670
421/600 [====================>.........] - ETA: 1:18 - loss: 0.7178 - acc: 0.7670
422/600 [====================>.........] - ETA: 1:18 - loss: 0.7173 - acc: 0.7671
423/600 [====================>.........] - ETA: 1:17 - loss: 0.7169 - acc: 0.7673
424/600 [====================>.........] - ETA: 1:17 - loss: 0.7161 - acc: 0.7675
425/600 [====================>.........] - ETA: 1:17 - loss: 0.7158 - acc: 0.7676
426/600 [====================>.........] - ETA: 1:16 - loss: 0.7154 - acc: 0.7677
427/600 [====================>.........] - ETA: 1:16 - loss: 0.7150 - acc: 0.7678
428/600 [====================>.........] - ETA: 1:15 - loss: 0.7144 - acc: 0.7679
429/600 [====================>.........] - ETA: 1:15 - loss: 0.7144 - acc: 0.7679
430/600 [====================>.........] - ETA: 1:14 - loss: 0.7140 - acc: 0.7680
431/600 [====================>.........] - ETA: 1:14 - loss: 0.7138 - acc: 0.7681
432/600 [====================>.........] - ETA: 1:13 - loss: 0.7137 - acc: 0.7681
433/600 [====================>.........] - ETA: 1:13 - loss: 0.7133 - acc: 0.7682
434/600 [====================>.........] - ETA: 1:13 - loss: 0.7131 - acc: 0.7684
435/600 [====================>.........] - ETA: 1:12 - loss: 0.7125 - acc: 0.7686
436/600 [====================>.........] - ETA: 1:12 - loss: 0.7120 - acc: 0.7687
437/600 [====================>.........] - ETA: 1:11 - loss: 0.7119 - acc: 0.7687
438/600 [====================>.........] - ETA: 1:11 - loss: 0.7116 - acc: 0.7688
439/600 [====================>.........] - ETA: 1:10 - loss: 0.7112 - acc: 0.7689
440/600 [=====================>........] - ETA: 1:10 - loss: 0.7111 - acc: 0.7689
441/600 [=====================>........] - ETA: 1:09 - loss: 0.7105 - acc: 0.7691
442/600 [=====================>........] - ETA: 1:09 - loss: 0.7102 - acc: 0.7693
443/600 [=====================>........] - ETA: 1:09 - loss: 0.7096 - acc: 0.7694
444/600 [=====================>........] - ETA: 1:08 - loss: 0.7093 - acc: 0.7696
445/600 [=====================>........] - ETA: 1:08 - loss: 0.7089 - acc: 0.7697
446/600 [=====================>........] - ETA: 1:07 - loss: 0.7087 - acc: 0.7698
447/600 [=====================>........] - ETA: 1:07 - loss: 0.7082 - acc: 0.7699
448/600 [=====================>........] - ETA: 1:06 - loss: 0.7077 - acc: 0.7701
449/600 [=====================>........] - ETA: 1:06 - loss: 0.7074 - acc: 0.7702
450/600 [=====================>........] - ETA: 1:05 - loss: 0.7071 - acc: 0.7703
451/600 [=====================>........] - ETA: 1:05 - loss: 0.7065 - acc: 0.7705
452/600 [=====================>........] - ETA: 1:05 - loss: 0.7062 - acc: 0.7706
453/600 [=====================>........] - ETA: 1:04 - loss: 0.7058 - acc: 0.7708
454/600 [=====================>........] - ETA: 1:04 - loss: 0.7057 - acc: 0.7708
455/600 [=====================>........] - ETA: 1:03 - loss: 0.7052 - acc: 0.7710
456/600 [=====================>........] - ETA: 1:03 - loss: 0.7049 - acc: 0.7711
457/600 [=====================>........] - ETA: 1:02 - loss: 0.7044 - acc: 0.7712
458/600 [=====================>........] - ETA: 1:02 - loss: 0.7040 - acc: 0.7713
459/600 [=====================>........] - ETA: 1:01 - loss: 0.7040 - acc: 0.7712
460/600 [======================>.......] - ETA: 1:01 - loss: 0.7035 - acc: 0.7715
461/600 [======================>.......] - ETA: 1:01 - loss: 0.7040 - acc: 0.7712
462/600 [======================>.......] - ETA: 1:00 - loss: 0.7037 - acc: 0.7713
463/600 [======================>.......] - ETA: 1:00 - loss: 0.7031 - acc: 0.7716
464/600 [======================>.......] - ETA: 59s - loss: 0.7027 - acc: 0.7716 
465/600 [======================>.......] - ETA: 59s - loss: 0.7024 - acc: 0.7716
466/600 [======================>.......] - ETA: 58s - loss: 0.7020 - acc: 0.7717
467/600 [======================>.......] - ETA: 58s - loss: 0.7016 - acc: 0.7719
468/600 [======================>.......] - ETA: 57s - loss: 0.7012 - acc: 0.7721
469/600 [======================>.......] - ETA: 57s - loss: 0.7007 - acc: 0.7723
470/600 [======================>.......] - ETA: 57s - loss: 0.7002 - acc: 0.7726
471/600 [======================>.......] - ETA: 56s - loss: 0.6999 - acc: 0.7727
472/600 [======================>.......] - ETA: 56s - loss: 0.6997 - acc: 0.7727
473/600 [======================>.......] - ETA: 55s - loss: 0.6992 - acc: 0.7729
474/600 [======================>.......] - ETA: 55s - loss: 0.6988 - acc: 0.7731
475/600 [======================>.......] - ETA: 54s - loss: 0.6985 - acc: 0.7731
476/600 [======================>.......] - ETA: 54s - loss: 0.6980 - acc: 0.7732
477/600 [======================>.......] - ETA: 53s - loss: 0.6978 - acc: 0.7732
478/600 [======================>.......] - ETA: 53s - loss: 0.6977 - acc: 0.7733
479/600 [======================>.......] - ETA: 53s - loss: 0.6973 - acc: 0.7735
480/600 [=======================>......] - ETA: 52s - loss: 0.6968 - acc: 0.7736
481/600 [=======================>......] - ETA: 52s - loss: 0.6967 - acc: 0.7736
482/600 [=======================>......] - ETA: 51s - loss: 0.6964 - acc: 0.7737
483/600 [=======================>......] - ETA: 51s - loss: 0.6959 - acc: 0.7739
484/600 [=======================>......] - ETA: 50s - loss: 0.6954 - acc: 0.7741
485/600 [=======================>......] - ETA: 50s - loss: 0.6951 - acc: 0.7742
486/600 [=======================>......] - ETA: 50s - loss: 0.6951 - acc: 0.7741
487/600 [=======================>......] - ETA: 49s - loss: 0.6947 - acc: 0.7742
488/600 [=======================>......] - ETA: 49s - loss: 0.6946 - acc: 0.7742
489/600 [=======================>......] - ETA: 48s - loss: 0.6941 - acc: 0.7745
490/600 [=======================>......] - ETA: 48s - loss: 0.6938 - acc: 0.7745
491/600 [=======================>......] - ETA: 47s - loss: 0.6937 - acc: 0.7747
492/600 [=======================>......] - ETA: 47s - loss: 0.6934 - acc: 0.7748
493/600 [=======================>......] - ETA: 46s - loss: 0.6932 - acc: 0.7748
494/600 [=======================>......] - ETA: 46s - loss: 0.6928 - acc: 0.7749
495/600 [=======================>......] - ETA: 46s - loss: 0.6925 - acc: 0.7750
496/600 [=======================>......] - ETA: 45s - loss: 0.6924 - acc: 0.7750
497/600 [=======================>......] - ETA: 45s - loss: 0.6922 - acc: 0.7751
498/600 [=======================>......] - ETA: 44s - loss: 0.6920 - acc: 0.7751
499/600 [=======================>......] - ETA: 44s - loss: 0.6915 - acc: 0.7753
500/600 [========================>.....] - ETA: 43s - loss: 0.6914 - acc: 0.7754
501/600 [========================>.....] - ETA: 43s - loss: 0.6912 - acc: 0.7756
502/600 [========================>.....] - ETA: 42s - loss: 0.6910 - acc: 0.7756
503/600 [========================>.....] - ETA: 42s - loss: 0.6905 - acc: 0.7758
504/600 [========================>.....] - ETA: 42s - loss: 0.6904 - acc: 0.7758
505/600 [========================>.....] - ETA: 41s - loss: 0.6905 - acc: 0.7758
506/600 [========================>.....] - ETA: 41s - loss: 0.6903 - acc: 0.7759
507/600 [========================>.....] - ETA: 40s - loss: 0.6898 - acc: 0.7761
508/600 [========================>.....] - ETA: 40s - loss: 0.6896 - acc: 0.7762
509/600 [========================>.....] - ETA: 39s - loss: 0.6890 - acc: 0.7764
510/600 [========================>.....] - ETA: 39s - loss: 0.6887 - acc: 0.7765
511/600 [========================>.....] - ETA: 38s - loss: 0.6884 - acc: 0.7765
512/600 [========================>.....] - ETA: 38s - loss: 0.6884 - acc: 0.7765
513/600 [========================>.....] - ETA: 38s - loss: 0.6881 - acc: 0.7766
514/600 [========================>.....] - ETA: 37s - loss: 0.6876 - acc: 0.7768
515/600 [========================>.....] - ETA: 37s - loss: 0.6872 - acc: 0.7769
516/600 [========================>.....] - ETA: 36s - loss: 0.6869 - acc: 0.7770
517/600 [========================>.....] - ETA: 36s - loss: 0.6868 - acc: 0.7769
518/600 [========================>.....] - ETA: 35s - loss: 0.6864 - acc: 0.7771
519/600 [========================>.....] - ETA: 35s - loss: 0.6859 - acc: 0.7772
520/600 [=========================>....] - ETA: 35s - loss: 0.6855 - acc: 0.7773
521/600 [=========================>....] - ETA: 34s - loss: 0.6854 - acc: 0.7774
522/600 [=========================>....] - ETA: 34s - loss: 0.6849 - acc: 0.7776
523/600 [=========================>....] - ETA: 33s - loss: 0.6850 - acc: 0.7776
524/600 [=========================>....] - ETA: 33s - loss: 0.6849 - acc: 0.7775
525/600 [=========================>....] - ETA: 32s - loss: 0.6845 - acc: 0.7777
526/600 [=========================>....] - ETA: 32s - loss: 0.6844 - acc: 0.7777
527/600 [=========================>....] - ETA: 31s - loss: 0.6842 - acc: 0.7778
528/600 [=========================>....] - ETA: 31s - loss: 0.6837 - acc: 0.7779
529/600 [=========================>....] - ETA: 31s - loss: 0.6833 - acc: 0.7781
530/600 [=========================>....] - ETA: 30s - loss: 0.6831 - acc: 0.7781
531/600 [=========================>....] - ETA: 30s - loss: 0.6827 - acc: 0.7783
532/600 [=========================>....] - ETA: 29s - loss: 0.6822 - acc: 0.7784
533/600 [=========================>....] - ETA: 29s - loss: 0.6817 - acc: 0.7786
534/600 [=========================>....] - ETA: 28s - loss: 0.6816 - acc: 0.7787
535/600 [=========================>....] - ETA: 28s - loss: 0.6812 - acc: 0.7788
536/600 [=========================>....] - ETA: 28s - loss: 0.6809 - acc: 0.7790
537/600 [=========================>....] - ETA: 27s - loss: 0.6808 - acc: 0.7791
538/600 [=========================>....] - ETA: 27s - loss: 0.6808 - acc: 0.7791
539/600 [=========================>....] - ETA: 26s - loss: 0.6807 - acc: 0.7791
540/600 [==========================>...] - ETA: 26s - loss: 0.6803 - acc: 0.7792
541/600 [==========================>...] - ETA: 25s - loss: 0.6801 - acc: 0.7793
542/600 [==========================>...] - ETA: 25s - loss: 0.6797 - acc: 0.7795
543/600 [==========================>...] - ETA: 24s - loss: 0.6795 - acc: 0.7795
544/600 [==========================>...] - ETA: 24s - loss: 0.6792 - acc: 0.7797
545/600 [==========================>...] - ETA: 24s - loss: 0.6789 - acc: 0.7798
546/600 [==========================>...] - ETA: 23s - loss: 0.6787 - acc: 0.7800
547/600 [==========================>...] - ETA: 23s - loss: 0.6785 - acc: 0.7801
548/600 [==========================>...] - ETA: 22s - loss: 0.6783 - acc: 0.7802
549/600 [==========================>...] - ETA: 22s - loss: 0.6779 - acc: 0.7803
550/600 [==========================>...] - ETA: 21s - loss: 0.6777 - acc: 0.7804
551/600 [==========================>...] - ETA: 21s - loss: 0.6776 - acc: 0.7804
552/600 [==========================>...] - ETA: 20s - loss: 0.6772 - acc: 0.7806
553/600 [==========================>...] - ETA: 20s - loss: 0.6769 - acc: 0.7807
554/600 [==========================>...] - ETA: 20s - loss: 0.6765 - acc: 0.7808
555/600 [==========================>...] - ETA: 19s - loss: 0.6763 - acc: 0.7809
556/600 [==========================>...] - ETA: 19s - loss: 0.6760 - acc: 0.7810
557/600 [==========================>...] - ETA: 18s - loss: 0.6758 - acc: 0.7810
558/600 [==========================>...] - ETA: 18s - loss: 0.6754 - acc: 0.7812
559/600 [==========================>...] - ETA: 17s - loss: 0.6752 - acc: 0.7812
560/600 [===========================>..] - ETA: 17s - loss: 0.6750 - acc: 0.7812
561/600 [===========================>..] - ETA: 17s - loss: 0.6746 - acc: 0.7814
562/600 [===========================>..] - ETA: 16s - loss: 0.6744 - acc: 0.7815
563/600 [===========================>..] - ETA: 16s - loss: 0.6742 - acc: 0.7816
564/600 [===========================>..] - ETA: 15s - loss: 0.6739 - acc: 0.7817
565/600 [===========================>..] - ETA: 15s - loss: 0.6734 - acc: 0.7819
566/600 [===========================>..] - ETA: 14s - loss: 0.6730 - acc: 0.7820
567/600 [===========================>..] - ETA: 14s - loss: 0.6726 - acc: 0.7822
568/600 [===========================>..] - ETA: 13s - loss: 0.6725 - acc: 0.7822
569/600 [===========================>..] - ETA: 13s - loss: 0.6719 - acc: 0.7824
570/600 [===========================>..] - ETA: 13s - loss: 0.6717 - acc: 0.7825
571/600 [===========================>..] - ETA: 12s - loss: 0.6714 - acc: 0.7825
572/600 [===========================>..] - ETA: 12s - loss: 0.6709 - acc: 0.7827
573/600 [===========================>..] - ETA: 11s - loss: 0.6708 - acc: 0.7828
574/600 [===========================>..] - ETA: 11s - loss: 0.6707 - acc: 0.7827
575/600 [===========================>..] - ETA: 10s - loss: 0.6703 - acc: 0.7828
576/600 [===========================>..] - ETA: 10s - loss: 0.6701 - acc: 0.7829
577/600 [===========================>..] - ETA: 10s - loss: 0.6697 - acc: 0.7831
578/600 [===========================>..] - ETA: 9s - loss: 0.6693 - acc: 0.7833 
579/600 [===========================>..] - ETA: 9s - loss: 0.6690 - acc: 0.7834
580/600 [============================>.] - ETA: 8s - loss: 0.6688 - acc: 0.7835
581/600 [============================>.] - ETA: 8s - loss: 0.6687 - acc: 0.7834
582/600 [============================>.] - ETA: 7s - loss: 0.6686 - acc: 0.7835
583/600 [============================>.] - ETA: 7s - loss: 0.6685 - acc: 0.7834
584/600 [============================>.] - ETA: 6s - loss: 0.6681 - acc: 0.7835
585/600 [============================>.] - ETA: 6s - loss: 0.6678 - acc: 0.7836
586/600 [============================>.] - ETA: 6s - loss: 0.6675 - acc: 0.7837
587/600 [============================>.] - ETA: 5s - loss: 0.6670 - acc: 0.7839
588/600 [============================>.] - ETA: 5s - loss: 0.6670 - acc: 0.7839
589/600 [============================>.] - ETA: 4s - loss: 0.6667 - acc: 0.7840
590/600 [============================>.] - ETA: 4s - loss: 0.6666 - acc: 0.7841
591/600 [============================>.] - ETA: 3s - loss: 0.6663 - acc: 0.7841
592/600 [============================>.] - ETA: 3s - loss: 0.6663 - acc: 0.7842
593/600 [============================>.] - ETA: 3s - loss: 0.6662 - acc: 0.7842
594/600 [============================>.] - ETA: 2s - loss: 0.6658 - acc: 0.7843
595/600 [============================>.] - ETA: 2s - loss: 0.6658 - acc: 0.7842
596/600 [============================>.] - ETA: 1s - loss: 0.6659 - acc: 0.7841
597/600 [============================>.] - ETA: 1s - loss: 0.6658 - acc: 0.7841
598/600 [============================>.] - ETA: 0s - loss: 0.6655 - acc: 0.7842
599/600 [============================>.] - ETA: 0s - loss: 0.6655 - acc: 0.7841
600/600 [==============================] - 277s 462ms/step - loss: 0.6655 - acc: 0.7841 - val_loss: 1.0138 - val_acc: 0.6996
/home/jamesbrace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

Epoch 00001: val_acc improved from -inf to 0.69960, saving model to weights/DenseNet-40-12-CIFAR10.h5
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7
Traceback (most recent call last):
  File "FashionMNIST/fashion_mnist.py", line 100, in <module>
    validation_steps=testX.shape[0] // batch_size, verbose=1)
  File "/home/jamesbrace/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/jamesbrace/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2262, in fit_generator
    callbacks.on_epoch_end(epoch, epoch_logs)
  File "/home/jamesbrace/anaconda3/lib/python3.6/site-packages/keras/callbacks.py", line 77, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File "/home/jamesbrace/anaconda3/lib/python3.6/site-packages/keras/callbacks.py", line 445, in on_epoch_end
    self.model.save_weights(filepath, overwrite=True)
  File "/home/jamesbrace/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2610, in save_weights
    with h5py.File(filepath, 'w') as f:
  File "/home/jamesbrace/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "/home/jamesbrace/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py", line 105, in make_fid
    fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 98, in h5py.h5f.create
OSError: Unable to create file (unable to open file: name = 'weights/DenseNet-40-12-CIFAR10.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)
2018-04-18 18:59:38.670130: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-04-18 18:59:38.745406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-04-18 18:59:38.745818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-04-18 18:59:38.745845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2018-04-18 18:59:38.946698: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7

2018-04-18 18:59:38.946923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2018-04-18 18:59:38.986680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 7709241234844119927
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 357171200
locality {
  bus_id: 1
}
incarnation: 16390130793262946728
physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7"
]
Model created
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 28, 28, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 28, 28, 24)   216         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 28, 28, 24)   96          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 28, 28, 24)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 28, 28, 12)   2592        activation_1[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 28, 28, 36)   0           conv2d_1[0][0]                   
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 28, 28, 36)   144         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 28, 28, 36)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 28, 28, 12)   3888        activation_2[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 28, 28, 48)   0           concatenate_1[0][0]              
                                                                 conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 28, 28, 48)   192         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 28, 28, 48)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 28, 28, 12)   5184        activation_3[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 28, 28, 60)   0           concatenate_2[0][0]              
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 28, 28, 60)   240         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 28, 28, 60)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 28, 28, 12)   6480        activation_4[0][0]               
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 28, 28, 72)   0           concatenate_3[0][0]              
                                                                 conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 28, 28, 72)   288         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 28, 28, 72)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 28, 12)   7776        activation_5[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 28, 28, 84)   0           concatenate_4[0][0]              
                                                                 conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 28, 84)   336         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 28, 84)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 12)   9072        activation_6[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 28, 28, 96)   0           concatenate_5[0][0]              
                                                                 conv2d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 96)   384         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 96)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 28, 28, 12)   10368       activation_7[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 28, 28, 108)  0           concatenate_6[0][0]              
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 28, 28, 108)  432         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 28, 28, 108)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 28, 28, 12)   11664       activation_8[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 28, 28, 120)  0           concatenate_7[0][0]              
                                                                 conv2d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 28, 28, 120)  480         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 28, 28, 120)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 28, 28, 12)   12960       activation_9[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 28, 28, 132)  0           concatenate_8[0][0]              
                                                                 conv2d_10[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 28, 28, 132)  528         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 28, 28, 132)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 28, 28, 12)   14256       activation_10[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 28, 28, 144)  0           concatenate_9[0][0]              
                                                                 conv2d_11[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 28, 28, 144)  576         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 28, 28, 144)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 28, 28, 12)   15552       activation_11[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 28, 28, 156)  0           concatenate_10[0][0]             
                                                                 conv2d_12[0][0]                  
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 28, 28, 156)  624         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 28, 28, 156)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 28, 28, 12)   16848       activation_12[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 28, 28, 168)  0           concatenate_11[0][0]             
                                                                 conv2d_13[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 28, 28, 168)  672         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 28, 28, 168)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 28, 28, 168)  28224       activation_13[0][0]              
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 14, 14, 168)  0           conv2d_14[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 14, 14, 168)  672         average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 14, 14, 168)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 14, 14, 12)   18144       activation_14[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 14, 14, 180)  0           average_pooling2d_1[0][0]        
                                                                 conv2d_15[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 14, 14, 180)  720         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 14, 14, 180)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 14, 14, 12)   19440       activation_15[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 14, 14, 192)  0           concatenate_13[0][0]             
                                                                 conv2d_16[0][0]                  
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 14, 14, 192)  768         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 14, 14, 192)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 14, 14, 12)   20736       activation_16[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 14, 14, 204)  0           concatenate_14[0][0]             
                                                                 conv2d_17[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 14, 14, 204)  816         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 14, 14, 204)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 14, 14, 12)   22032       activation_17[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 14, 14, 216)  0           concatenate_15[0][0]             
                                                                 conv2d_18[0][0]                  
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 14, 14, 216)  864         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 14, 14, 216)  0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 14, 14, 12)   23328       activation_18[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 14, 14, 228)  0           concatenate_16[0][0]             
                                                                 conv2d_19[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 14, 14, 228)  912         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 14, 14, 228)  0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 14, 14, 12)   24624       activation_19[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 14, 14, 240)  0           concatenate_17[0][0]             
                                                                 conv2d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 14, 14, 240)  960         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 14, 14, 240)  0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 14, 14, 12)   25920       activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 14, 14, 252)  0           concatenate_18[0][0]             
                                                                 conv2d_21[0][0]                  
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 14, 14, 252)  1008        concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 14, 14, 252)  0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 14, 14, 12)   27216       activation_21[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 14, 14, 264)  0           concatenate_19[0][0]             
                                                                 conv2d_22[0][0]                  
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 14, 14, 264)  1056        concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 14, 14, 264)  0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 14, 14, 12)   28512       activation_22[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 14, 14, 276)  0           concatenate_20[0][0]             
                                                                 conv2d_23[0][0]                  
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 14, 14, 276)  1104        concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 14, 14, 276)  0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 14, 14, 12)   29808       activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 14, 14, 288)  0           concatenate_21[0][0]             
                                                                 conv2d_24[0][0]                  
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 14, 14, 288)  1152        concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 14, 14, 288)  0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 14, 14, 12)   31104       activation_24[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 14, 14, 300)  0           concatenate_22[0][0]             
                                                                 conv2d_25[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 14, 14, 300)  1200        concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 14, 14, 300)  0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 14, 14, 12)   32400       activation_25[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 14, 14, 312)  0           concatenate_23[0][0]             
                                                                 conv2d_26[0][0]                  
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 14, 14, 312)  1248        concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 14, 14, 312)  0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 14, 14, 312)  97344       activation_26[0][0]              
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 7, 7, 312)    0           conv2d_27[0][0]                  
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 7, 7, 312)    1248        average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 7, 7, 312)    0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 7, 7, 12)     33696       activation_27[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 7, 7, 324)    0           average_pooling2d_2[0][0]        
                                                                 conv2d_28[0][0]                  
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 7, 7, 324)    1296        concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 7, 7, 324)    0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 7, 7, 12)     34992       activation_28[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 7, 7, 336)    0           concatenate_25[0][0]             
                                                                 conv2d_29[0][0]                  
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 7, 7, 336)    1344        concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 7, 7, 336)    0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 7, 7, 12)     36288       activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 7, 7, 348)    0           concatenate_26[0][0]             
                                                                 conv2d_30[0][0]                  
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 7, 7, 348)    1392        concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 7, 7, 348)    0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 7, 7, 12)     37584       activation_30[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 7, 7, 360)    0           concatenate_27[0][0]             
                                                                 conv2d_31[0][0]                  
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 7, 7, 360)    1440        concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 7, 7, 360)    0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 7, 7, 12)     38880       activation_31[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 7, 7, 372)    0           concatenate_28[0][0]             
                                                                 conv2d_32[0][0]                  
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 7, 7, 372)    1488        concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 7, 7, 372)    0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 7, 7, 12)     40176       activation_32[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 7, 7, 384)    0           concatenate_29[0][0]             
                                                                 conv2d_33[0][0]                  
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 7, 7, 384)    1536        concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 7, 7, 384)    0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 7, 7, 12)     41472       activation_33[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 7, 7, 396)    0           concatenate_30[0][0]             
                                                                 conv2d_34[0][0]                  
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 7, 7, 396)    1584        concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 7, 7, 396)    0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 7, 7, 12)     42768       activation_34[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 7, 7, 408)    0           concatenate_31[0][0]             
                                                                 conv2d_35[0][0]                  
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 7, 7, 408)    1632        concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 7, 7, 408)    0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 7, 7, 12)     44064       activation_35[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 7, 7, 420)    0           concatenate_32[0][0]             
                                                                 conv2d_36[0][0]                  
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 7, 7, 420)    1680        concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 7, 7, 420)    0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 7, 7, 12)     45360       activation_36[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 7, 7, 432)    0           concatenate_33[0][0]             
                                                                 conv2d_37[0][0]                  
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 7, 7, 432)    1728        concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 7, 7, 432)    0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 7, 7, 12)     46656       activation_37[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 7, 7, 444)    0           concatenate_34[0][0]             
                                                                 conv2d_38[0][0]                  
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 7, 7, 444)    1776        concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 7, 7, 444)    0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 7, 7, 12)     47952       activation_38[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 7, 7, 456)    0           concatenate_35[0][0]             
                                                                 conv2d_39[0][0]                  
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 7, 7, 456)    1824        concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 7, 7, 456)    0           batch_normalization_39[0][0]     
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 456)          0           activation_39[0][0]              
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           4570        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 1,077,586
Trainable params: 1,058,866
Non-trainable params: 18,720
__________________________________________________________________________________________________
Finished compiling
Building model...
Training with data augmentation...
Epoch 1/40

  1/600 [..............................] - ETA: 1:02:59 - loss: 2.4950 - acc: 0.1000
  2/600 [..............................] - ETA: 33:36 - loss: 2.3188 - acc: 0.1750  
  3/600 [..............................] - ETA: 23:46 - loss: 2.1365 - acc: 0.2567
  4/600 [..............................] - ETA: 18:51 - loss: 1.9902 - acc: 0.3075
  5/600 [..............................] - ETA: 15:54 - loss: 1.9077 - acc: 0.3280
  6/600 [..............................] - ETA: 13:56 - loss: 1.8380 - acc: 0.3667
  7/600 [..............................] - ETA: 12:31 - loss: 1.7739 - acc: 0.3857
  8/600 [..............................] - ETA: 11:28 - loss: 1.7242 - acc: 0.4063
  9/600 [..............................] - ETA: 10:38 - loss: 1.6722 - acc: 0.42442018-04-18 19:09:53.609994: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-04-18 19:09:53.684803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-04-18 19:09:53.685159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-04-18 19:09:53.685186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2018-04-18 19:09:53.883255: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7

2018-04-18 19:09:53.883539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2018-04-18 19:09:53.924538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
